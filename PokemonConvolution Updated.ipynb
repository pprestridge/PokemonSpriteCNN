{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff90759",
   "metadata": {},
   "source": [
    "# Pokemon Convolution\n",
    "\n",
    "This Jupyter Notebook will explore predicting information about a Pokemon based on their sprite. \n",
    "\n",
    "Steps:\n",
    "1) WebScrape together a dataset of Pokemon images\n",
    "\n",
    "2) Develop and Train a convolutional model to classify Pokemon by their type\n",
    "\n",
    "3) Develop and Train a regression model to predict the stats of a Pokemon\n",
    "\n",
    "4) Put it all together into a single function that plots the sprite and model predictions agaisnt the ground truth\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3610fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import shutils\n",
    "import tqdm\n",
    "import random\n",
    "import PIL\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d3dd6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Scrape and Store the Data\n",
    "\n",
    "Define a buildDatabase function to import all the pokemon name, typings, and a link to download their sprite and store it in a Pandas DataFrame. Thanks to Pokemondb.net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDatabase(url):\n",
    "\n",
    "    # Pull in the url's html\n",
    "    webpage = requests.get(url)\n",
    "    \n",
    "    # Use Beautiful Soup to parse through the html\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    \n",
    "    # Navigate the soup datastructure to sort through each pokemon to pull its information\n",
    "    typing_list = []\n",
    "    for entry in soup.find_all(\"div\", class_=\"infocard\"):\n",
    "        \n",
    "        # Identify the Pokemon's name and typing\n",
    "        pokemon = entry.find_all(\"a\", class_ = True)\n",
    "        for i, item in enumerate(pokemon):\n",
    "            \n",
    "            # Record the name\n",
    "            item = item[\"href\"].split(\"/\")\n",
    "            if item[1] == 'pokedex':\n",
    "                name = item[2]\n",
    "            \n",
    "            # Record the typing\n",
    "            if item[1] =='type':\n",
    "                typing = item[2]\n",
    "                typing_list.append(typing)\n",
    "                \n",
    "        # Handle dual and single typing\n",
    "        if len(typing_list) == 2:\n",
    "            typing1 = typing_list[0]\n",
    "            typing2 = typing_list[1]\n",
    "            typing_list = []\n",
    "        if len(typing_list) == 1:\n",
    "            typing1 = typing_list[0]\n",
    "            typing2 = \"None\"\n",
    "            typing_list = []\n",
    "        \n",
    "        # End Pokemon collection after this point due to change in sprite quality\n",
    "        if name ==\"meltan\":\n",
    "            break\n",
    "        \n",
    "        # Identify the Pokemon's sprites image link  \n",
    "        link = entry.find(\"span\", class_=\"img-fixed img-sprite\")\n",
    "        link = link[\"data-src\"]\n",
    "        \n",
    "        # Store the Pokemon's information in the DataFrame\n",
    "        entry_dict = {'Pokemon':name,\n",
    "                     'Type 1' : typing1,\n",
    "                     'Type 2' : typing2,\n",
    "                     'ImageLink' : link}\n",
    "      \n",
    "        if name != 'bulbasaur':\n",
    "            pokemonDataFrame = pokemonDataFrame.append(entry_dict, ignore_index=True)\n",
    "        else:\n",
    "            pokemonDataFrame = pd.DataFrame(data=entry_dict, index=[0])\n",
    "    \n",
    "    # With a list of all of the pokemon, we will have to use another website to gather all of their stats\n",
    "    hp = []\n",
    "    attack = []\n",
    "    defense = []\n",
    "    sp_attack = []\n",
    "    sp_defense = []\n",
    "    speed = []\n",
    "    \n",
    "    # Cycle through the seperate website for each pokemon\n",
    "    for i in range(len(pokemonDataFrame)):\n",
    "        \n",
    "        # Define Pokemon specific website url\n",
    "        url = 'https://pokemondb.net/pokedex/' +  pokemonDataFrame[\"Pokemon\"][i]\n",
    "        \n",
    "        # Pull in the url's html\n",
    "        webpage = requests.get(url)\n",
    "\n",
    "        # Use Beautiful Soup to parse through the html\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        \n",
    "        # Sort through to find the append the base stat to the list\n",
    "        for j, stat in enumerate(soup.find_all(\"td\", class_=\"cell-num\")):\n",
    "            if j == 0:\n",
    "                hp.append(stat.text)\n",
    "            if j == 3:\n",
    "                attack.append(stat.text)\n",
    "            if j == 6:\n",
    "                defense.append(stat.text)\n",
    "            if j == 9:\n",
    "                sp_attack.append(stat.text)\n",
    "            if j == 12:\n",
    "                sp_defense.append(stat.text)\n",
    "            if j == 15:\n",
    "                speed.append(stat.text)\n",
    "    \n",
    "    # Create a Data Frame with the stats and attach it to the existing Data Frame\n",
    "    stats_dict = {'HP': hp,\n",
    "            'Attack' : attack,\n",
    "            'Defense' : defense,\n",
    "            'Special Attack' : sp_attack,\n",
    "            'Special Defense' : sp_defense,\n",
    "            'Speed' : speed,}\n",
    "    \n",
    "    # Create the stats Data Frame\n",
    "    statsDatatFrame = pd.DataFrame(data=stats_dict)\n",
    "    \n",
    "    # Attach it to the existing Data Frame\n",
    "    pokemonDataFrame = pd.concat([pokemonDataFrame, statsDatatFrame], axis=1)        \n",
    "    \n",
    "    return pokemonDataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611cfc2b",
   "metadata": {},
   "source": [
    "Let's run the function to create our DataFrame and check out an entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cdf332",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pokemonDataFrame = buildDatabase('https://pokemondb.net/pokedex/national')\n",
    "print(pokemonDataFrame.iloc[156])\n",
    "\n",
    "pokemonDataFrame.to_pickle(\"./PokemonDataFrame.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4121fd",
   "metadata": {},
   "source": [
    "## Develop Image Dataset\n",
    "\n",
    "Now that we have a Data Frame with all of the pokemon we plan to train and test our model with, we need to go through and download the sprite image for each pokemon and save it to a local folder. I started by webscrapping all of the images from the most recent generation, which resulted in ~800 training examples for 20 different classes. The thinking was that higher resolution training examples would provide better training data; however, that decision proved poor. It was challenging to train the model with that few training examples, so I went back and downloaded more from older generations to have closer to 6k training examples.  To further augment the data, the convolutional model included a random horizontal flip and a random rotation. Images of the current training dataset can be seen below Images are saved based on their classification, which will allow TensorFlow to load the entire Dataset, both input and classification, in with one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spriteDownloaderwithType(df):\n",
    "\n",
    "    # Go down each entry of the dataframe and save the sprite's image from the \"ImageLink\" column\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        # Pull the url and define the save path\n",
    "        url = df[\"ImageLink\"].iloc[i]\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        \n",
    "        # Only handling primary typing           \n",
    "        typing = df[\"Type 1\"].iloc[i]\n",
    "                \n",
    "        # Determine Directory to save photo\n",
    "        path = os.getcwd() + \"\\PokemonSprites\\\\\" + typing + \"\\\\\"\n",
    "        \n",
    "        # Check if the correct path already exists\n",
    "        if os.path.isdir(path):\n",
    "            \n",
    "            # Save photo\n",
    "            response = requests.get(url)\n",
    "            with open(path+filename, 'wb') as local_file:\n",
    "                local_file.write(response.content)\n",
    "                \n",
    "         # Create the path if necessary\n",
    "        else:\n",
    "            new_directory = typing\n",
    "            directory = os.getcwd() + \"\\PokemonSprites\\\\\"\n",
    "            directory = os.path.join(directory, new_directory)\n",
    "            os.mkdir(directory)\n",
    "            \n",
    "            # Save photo\n",
    "            response = requests.get(url)\n",
    "            with open(path+filename, 'wb') as local_file:\n",
    "                local_file.write(response.content)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda51061",
   "metadata": {},
   "outputs": [],
   "source": [
    "spriteDownloaderwithType(pokemonDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4683b",
   "metadata": {},
   "source": [
    "At this point, we have collected all of the data to train and test our model. All of the images are stored in a local folder. Let's start by defining the image size and datasets(train/dev/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tf.Dataset parameters\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)\n",
    "IMG_SHAPE = (128, 128,3)\n",
    "\n",
    "# Define file path\n",
    "trainDir = os.getcwd() + \"\\\\cleaned_train\"\n",
    "testDir = os.getcwd() + \"\\\\cleaned_test\"\n",
    "seed = 107\n",
    "\n",
    "# Load in the training set and apply augmentation techniques\n",
    "trainDataGenerator = ImageDataGenerator(rescale = 1.0/255.0,\n",
    "                                       rotation_range = 90,\n",
    "                                       zoom_range = 0.2,\n",
    "                                       shear_range = 0.2,\n",
    "                                       horizontal_flip = True)\n",
    "\n",
    "trainDataGenerator = trainDataGenerator.flow_from_directory(trainDir,\n",
    "                                                           target_size = IMG_SIZE,\n",
    "                                                           class_mode = \"categorical\",\n",
    "                                                           batch_size = BATCH_SIZE)\n",
    "\n",
    "# Load in the test set\n",
    "testDataGenerator = ImageDataGenerator(rescale = 1.0/255.0)\n",
    "\n",
    "testDataGenerator = testDataGenerator.flow_from_directory(testDir,\n",
    "                                                           target_size = IMG_SIZE,\n",
    "                                                           class_mode = \"categorical\",\n",
    "                                                           batch_size = BATCH_SIZE)\n",
    "\n",
    "classNames = trainDataGenerator.class_indices.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35890f0",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "The training data looks sufficient at this point. Now, we can look at developing the convolutional model. Given the size of the dataset and that Pokémon type recognition seemed akin to classifying animals at a zoo, transfer learned from readily available pre-trained networks seemed like an auspicious path. \n",
    "\n",
    "To speed up the model selection profess, I opted to reduce the image size down to 75x75 to weed our poorly performing models in favor of moe auspicious architectures. Then, once I was happy with the model, I updated image size to 128x128. Althought it drastically increased the training time, the accuracy increased by around 20% with the increased resolution. Since all of teh preliminary development was done on smaller files, I was able to consider more options quicker. Ultimately, I arrived at a validation set accuracy of ~76%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abf6ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    \n",
    "    # Load in a pretrained InceptionNetV3\n",
    "    ptModel = tf.keras.applications.inception_v3.InceptionV3(include_top=False, input_shape=IMG_SHAPE, weights=\"imagenet\")\n",
    "    \n",
    "    # Freeze all the existing layers in the pretrained Inception Model\n",
    "    for layer in ptModel.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # For our model, we will pull from an intermediate layer, \"mixed7\" and use that as an input into our layers\n",
    "    LastLayer = ptModel.get_layer(\"mixed7\")\n",
    "    LastOutput = LastLayer.output\n",
    "    \n",
    "    # Reduce the output of the network with a (1,1) Convolution filter\n",
    "    x = tf.keras.layers.Conv2D(1024, 1)(LastOutput)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # Add 2 FC layers with Dropout\n",
    "    x = tf.keras.layers.Dropout(0.15)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.15)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    \n",
    "    # Add softmax output layer\n",
    "    x = tf.keras.layers.Dense(18, activation =\"softmax\")(x)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=ptModel.inputs, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeModel = buildModel()\n",
    "print(TypeModel.summary())\n",
    "\n",
    "TypeModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate =0.000005),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5d99b",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks to stop unauspicous combinatoins from completing all epochs and stop models once overfitting occurs\n",
    "stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            patience= 100, \n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = TypeModel.fit(trainDataGenerator,\n",
    "                        validation_data = testDataGenerator,\n",
    "                        epochs = 500, steps_per_epoch = 128,\n",
    "                        validation_batch_size = 128, validation_steps = 16,\n",
    "                        shuffle=True, callbacks = [stopping])\n",
    "\n",
    "# Save the model weighs\n",
    "directory = os.getcwd() + \"\\\\Model\\\\TunedModel\"\n",
    "TypeModel.save_weights(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd() + \"\\\\Model\\\\TunedModel\"\n",
    "TypeModel.load_weights(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd9345",
   "metadata": {},
   "source": [
    "Only 3 classes comprised 70% of the training data, which was one of the major issues. Another issue was the inconsistency in the training data. Not all images were of similar quality or same size. With more time, I would make a confusion matrix to better understand the error and work on image handling to improve the train and test dataset.  Developing a framework to understand the error was challenging, because in theory, the Bayes Error is 0%. There are people who know the typing of every Pokémon, but that's because there are a finite number of Pokémon and that can easily be memorized. On the other hand, the model is tasked with the task of classifying Pokémon it hasn't seen before, which explains the some of the avoidable bias. Although impossible to quantify, the error for experts completing the same task as the model would likely be greater than 0%. For such an arbitrary and meaningless task, it is challenging to put numbers to the type of error present in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba714496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plotter(history):\n",
    "    acc = [0.] + history.history['accuracy']\n",
    "    val_acc = [0.] + history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    loss = [0.] + history.history['loss']\n",
    "    val_loss = [0.] + history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([min(plt.ylim()),max(plt.ylim())])\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "history_plotter(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a287188",
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeModel.evaluate(testDataGenerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ba698",
   "metadata": {},
   "source": [
    "## Regression Model Development\n",
    "\n",
    "The next step is to develop a model that translates images into numerical values, representing the stats of a Pokémon. Starting from scratch by making another convolutional model would require more time doing model development. Instead, the classification model can be turned into an image encoder by removing the SoftMax layer. Then, the processed image encodings would be used to tune a simple sequential model.  The intuition is that layers early in a convolutional network pickup small and individual edges, and as the information propagates, the network develops these into larger scale or more meaningful pieces of information. So, the last dense layer in the classification model has taken the (128, 128, 3) image and distilled it into 512 factors that suffice to describe the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and remove the softmax layer to make it into an encoding network\n",
    "def EncodingModel(TypeModel):\n",
    "\n",
    "    # Get the pre-trained model and its weights\n",
    "    directory = os.getcwd() + \"\\\\Model\\\\TunedModel\"\n",
    "    TypeModel.load_weights(directory)\n",
    "    \n",
    "    # Update the model to remove the softmax layer.\n",
    "    x = TypeModel.layers[-2].output\n",
    "    \n",
    "    # Define output & model\n",
    "    TypeModel = tf.keras.Model(inputs=TypeModel.input, outputs=x)\n",
    "    \n",
    "    return TypeModel\n",
    "\n",
    "EncModel = EncodingModel(TypeModel)\n",
    "EncModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35702a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spriteEncoder(pokemonDataFrame,EncModel):\n",
    "\n",
    "    \n",
    "    # Cycle through each pokemon, pull its sprite, feed it to the encoding model, and story the output\n",
    "    for i in range(len(pokemonDataFrame)):\n",
    "        \n",
    "        # Define directory\n",
    "        pokemon = pokemonDataFrame[\"Pokemon\"][i] + \".png\"\n",
    "        directory = os.getcwd() + \"\\\\PokemonSpritesalphabetical\\\\\"\n",
    "        filepath = directory + pokemon\n",
    "        \n",
    "        # Load and format the image\n",
    "        image = tf.keras.preprocessing.image.load_img(filepath, grayscale=False, color_mode='rgb',interpolation='nearest')\n",
    "        input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        input_arr = np.expand_dims(input_arr, axis=0)\n",
    "        \n",
    "        # Process the image with a single forard propagation\n",
    "        encoding = EncModel.predict(x=input_arr)\n",
    "        \n",
    "        # Append the encoding to the X_data array\n",
    "        if i == 0:\n",
    "            X_data = np.array(encoding)\n",
    "        else:\n",
    "            X_data = np.vstack((X_data, encoding))\n",
    "    \n",
    "    # Return the X_Data and the Y_Data\n",
    "    Y_data = pokemonDataFrame[[\"HP\", \"Attack\", \"Defense\", \"Special Attack\", \"Special Defense\", \"Speed\"]]\n",
    "\n",
    "    \n",
    "    return X_data, Y_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75658301",
   "metadata": {},
   "source": [
    "Now that we have function to handle the encoding, we can develop our input and output data for the stat predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69880376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of input and output data\n",
    "X_data, Y_data = spriteEncoder(pokemonDataFrame,EncModel)\n",
    "\n",
    "# Check dimensions to ensure everything ran correctly\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)\n",
    "\n",
    "# Develop data in a TF Dataset -- disregarding good practice of train/dev/test splits\n",
    "Y_data = tf.strings.to_number(Y_data)\n",
    "statDataset = tf.data.Dataset.from_tensor_slices((X_data, Y_data)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f0156",
   "metadata": {},
   "source": [
    "For the regression model that handles stat predictions, a simple model with 3 dense layers and dropout will suffice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildStatModel():\n",
    "\n",
    "    # Define input\n",
    "    encoding_shape = (512)\n",
    "    input_encoding = tf.keras.Input(shape=encoding_shape)\n",
    "    \n",
    "    # Add 3 Dense layers with\n",
    "    x = tf.keras.layers.Dense(100, activation=\"relu\")(input_encoding)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x) \n",
    "    \n",
    "    # Define Output Layer\n",
    "    predictions = tf.keras.layers.Dense(6)(x)\n",
    "    \n",
    "    # NN tend to prefer smaller numbers, so applying a scaling factor here should help the model\n",
    "    predictions = tf.math.multiply(predictions, 10)\n",
    "    \n",
    "    # Return Model\n",
    "    statModel = tf.keras.Model(inputs=input_encoding, outputs=predictions)\n",
    "    \n",
    "    return statModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "statModel = buildStatModel()\n",
    "\n",
    "statModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate =0.0001),\n",
    "                 loss=\"mse\",\n",
    "                 metrics=[\"mae\"])\n",
    "\n",
    "statModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = statModel.fit(statDataset, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da3671",
   "metadata": {},
   "source": [
    "Putting this all together,  we have a model that takes an image of a Pokémon and predicts its typing and stats. One model handles the image encoding and classification, and the second model leverages the image encoding of the first model to predict its stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionChecker(number,pokemonDataFrame, statModel, TypeModel, X_data, Y_data, classNames):\n",
    "\n",
    "    \n",
    "    #Retrieve and show Sprite Image\n",
    "    pokemonDataFrame.iloc[number]\n",
    "    pokemon = pokemonDataFrame[\"Pokemon\"][number] + \".png\"\n",
    "    directory = os.getcwd() + \"\\\\PokemonSpritesalphabetical\\\\\"\n",
    "    filepath = directory + pokemon\n",
    "    image = PIL.Image.open(filepath)\n",
    "    \n",
    "    # Load and format the image\n",
    "    image = tf.keras.preprocessing.image.load_img(filepath, grayscale=False, color_mode='rgb',interpolation='nearest')\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    input_arr = np.expand_dims(input_arr, axis=0)\n",
    "    \n",
    "    # Predict Type\n",
    "    typePrediction = TypeModel.predict(input_arr, verbose=0)\n",
    "    print(typePrediction)\n",
    "    \n",
    "    # Predict Stats\n",
    "    input_arr = np.expand_dims(X_data[number,:], axis=0)\n",
    "    statPrediction = statModel.predict(input_arr)\n",
    "\n",
    "\n",
    "    #Plot Sprite\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title( pokemonDataFrame[\"Pokemon\"][number] +\"'s Sprite\")\n",
    "    \n",
    "    #Setup Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "    fig.suptitle(\"Model Predictions\")\n",
    "    \n",
    "    # Plot Type Prediction\n",
    "    typePred = pd.DataFrame(data=typePrediction.reshape(-1), index=classNames, columns=[\"Percentage of Confidence\"])\n",
    "    typePred = typePred.sort_values(by=\"Percentage of Confidence\", ascending=True)\n",
    "    typePred = typePred[14:]\n",
    "    typePred.plot.barh(ax=ax2)\n",
    "    plt.title(\"Correct Type: \" + pokemonDataFrame[\"Type 1\"][number])\n",
    "\n",
    "    \n",
    "    # Plot Stat Prediction\n",
    "    indices = [\"HP\", \"Attack\", \"Defense\", \"Special Attack\", \"Special Defense\", \"Speed\"]\n",
    "    actualStats = np.array(Y_data[number,:])\n",
    "    stats = np.hstack((actualStats.reshape(6,1), statPrediction.reshape(6,1)))\n",
    "    stats = pd.DataFrame(data=stats, index = indices, columns=[\"Actual Stats\", \"Predicted Stats\"])\n",
    "    stats.plot.barh(ax=ax1)\n",
    "\n",
    "    plt.savefig(\"figure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c3502",
   "metadata": {},
   "source": [
    "predictionChecker is the bow on top of this project. It will take the Pokedex number of a Pokemon show the model input and outpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bde273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction of a pokemon\n",
    "PokedexNumber = 4\n",
    "predictionChecker(PokedexNumber, pokemonDataFrame, statModel, TypeModel, X_data, Y_data, classNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c2220",
   "metadata": {},
   "source": [
    "## Test Myself\n",
    "\n",
    "Now that the formalities are out of the way, I can use this program for its intended use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00603fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve and Image of me\n",
    "filepath = os.getcwd() + \"\\\\Headshot.jpg\"\n",
    "image = PIL.Image.open(filepath)\n",
    "    \n",
    "# Load and format the image\n",
    "image = tf.keras.preprocessing.image.load_img(filepath, grayscale=False, color_mode='rgb', target_size=(128,128,3),interpolation='nearest')\n",
    "input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "input_arr = np.expand_dims(input_arr, axis=0)\n",
    "    \n",
    "# Predict Type\n",
    "typePrediction = TypeModel.predict(input_arr, verbose=0)\n",
    "    \n",
    "# Predict Stats\n",
    "encoding = EncModel.predict(x=input_arr)\n",
    "input_arr = np.expand_dims(encoding, axis=0)\n",
    "statPrediction = statModel.predict(encoding)\n",
    "\n",
    "\n",
    "#Plot Sprite\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title( \"Patrick's Sprite\")\n",
    "    \n",
    "#Setup Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "fig.suptitle(\"Model Predictions\")\n",
    "\n",
    "# Plot Type Prediction\n",
    "typePred = pd.DataFrame(data=typePrediction.reshape(-1), index=classNames, columns=[\"Percentage of Confidence\"])\n",
    "typePred = typePred.sort_values(by=\"Percentage of Confidence\", ascending=True)\n",
    "typePred = typePred[14:]\n",
    "typePred.plot.barh(ax=ax2)\n",
    "\n",
    "# Plot Stat Prediction\n",
    "indices = [\"HP\", \"Attack\", \"Defense\", \"Special Attack\", \"Special Defense\", \"Speed\"]\n",
    "stats = statPrediction.reshape(6,1)\n",
    "stats = pd.DataFrame(data=stats, index = indices, columns=[ \"Predicted Stats\"])\n",
    "stats.plot.barh(ax=ax1)\n",
    "ax1.legend([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c1682",
   "metadata": {},
   "source": [
    "I think the psychich typing is a compliment, hahaha!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e8784",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f944ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "pokemonDataFrame = pd.read_pickle(\"./PokemonDataFrame.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distibution of classes across the entire dataset (train and test)\n",
    "typeDistribution = pokemonDataFrame[\"Type 1\"].value_counts()\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.bar(height = typeDistribution[:], x = typeDistribution.index)\n",
    "plt.title(\"Data Set Type Distribution\")\n",
    "\n",
    "plt.savefig(\"typeDistribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f06286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the classic 3: Water, Grass, and Fire\n",
    "indicesW = pokemonDataFrame['Type 1'].str.contains('water')\n",
    "indicesG = pokemonDataFrame['Type 1'].str.contains('grass')\n",
    "indicesF = pokemonDataFrame['Type 1'].str.contains('fire')\n",
    "\n",
    "# Remove all other types to understand how accurate the type encodings are\n",
    "pokemonDataFrameSimple = pokemonDataFrame[indicesW + indicesG + indicesF]\n",
    "encodingSimple = X_data[indicesW + indicesG + indicesF,:]\n",
    "\n",
    "# Check sizes to ensure data was pruned correcly\n",
    "print(pokemonDataFrameSimple.shape)\n",
    "print(encodingSimple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6818b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a PCA on the Data to understand how variance is distributed by encoding components\n",
    "pca = PCA() \n",
    "pca.fit(encodingSimple)\n",
    "\n",
    "# Plot the retained variance against number of components\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "plt.plot( np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"CDF of Variance\")\n",
    "plt.ylabel(\"Percentage of Variance Explained\")\n",
    "plt.xlabel(\"Number of Variables\")\n",
    "plt.savefig(\"PCA\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053cb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Peform a PCA with 3 components to visualize the distinction between Fire, Water, and Grass encodings\n",
    "scaling = StandardScaler()\n",
    "scaling.fit(encodingSimple)\n",
    "encodingSimple = scaling.transform(encodingSimple)\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(encodingSimple)\n",
    "pcaEncodings = pca.transform(encodingSimple)\n",
    "\n",
    "\n",
    "\n",
    "# Make a 3D scatter plot\n",
    "maskW = pokemonDataFrameSimple[\"Type 1\"].str.contains('water')\n",
    "maskF = pokemonDataFrameSimple[\"Type 1\"].str.contains(\"grass\")\n",
    "maskG = pokemonDataFrameSimple[\"Type 1\"].str.contains(\"fire\")\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "wScat = ax.scatter(pcaEncodings[maskW][:,0],\n",
    "               pcaEncodings[maskW][:,1],\n",
    "               pcaEncodings[maskW][:,2],\n",
    "               'b')\n",
    "\n",
    "fScat = ax.scatter(pcaEncodings[maskF][:,0],\n",
    "               pcaEncodings[maskF][:,1],\n",
    "               pcaEncodings[maskF][:,2],\n",
    "               'r')\n",
    "\n",
    "gScat = ax.scatter(pcaEncodings[maskG][:,0],\n",
    "               pcaEncodings[maskG][:,1],\n",
    "               pcaEncodings[maskG][:,2],\n",
    "               'g')\n",
    "\n",
    "ax.legend([fScat, wScat, gScat], ['Fire', 'Water', 'Grass'])\n",
    "\n",
    "# Create a Gif of the \n",
    "ax.view_init(elev=45, azim = 0)\n",
    "def rotateZ(angle):\n",
    "    ax.view_init(azim=angle, elev=angle/5)\n",
    "    \n",
    "rot_animation = animation.FuncAnimation(fig, rotateZ, frames=np.arange(0, 270, 2), interval=100)\n",
    "rot_animation.save('rotation.gif', dpi=80, writer='imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcae17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.view_init(elev=10, azim = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
